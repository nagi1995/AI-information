# Dimensionality Reduction and Visualization

## This topic is placed here because, in real world problems, 100s of features used to build a model is common practice. To get early inslights and to reduce the features, we use Dimensionality Reduction techniques like PCA and t-SNE.


> Why do we need Dimensionality Reduction?
>
>  *   Dimensionality reduction helps with these problems, while trying to preserve most of the relevant information in the data needed to learn accurate, predictive models.
>  *  There are often too many factors on the basis of which the final prediction is done. These factors are basically variables called features.
>  *  The higher the number of features, the harder it gets to visualize the training set and then work on it.
>  *  Sometimes, most of these features are correlated, and hence redundant. This is where dimensionality reduction algorithms come into play.


> * Importance of Dimensionality reduction?
>
> *    It reduces the time and storage space required.
> *     It helps Remove multi-collinearity which improves the interpretation of the parameters of the machine learning model.
> *     It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.
> *     It avoids the curse of dimensionality.
> *     It removes irrelevant features from the data, Because having irrelevant features in the data can decrease the accuracy of the models and make your model learn based on irrelevant features.
> - [Source](https://medium.com/analytics-vidhya/importance-of-dimensionality-reduction-d6a4c7289b92)

## There are free videos on [PCA](https://www.youtube.com/watch?v=Zi94l9AjbLg&list=PLupD_xFct8mFgtBPBHAAmYbUgqDzzKOlP) and [t-SNE](https://www.youtube.com/watch?v=FQmCzpKWD48&list=PLupD_xFct8mHqCkuaXmeXhe0ajNDu0mhZ) on Applied AI Course youtube.
